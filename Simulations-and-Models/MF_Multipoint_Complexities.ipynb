{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2148b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Imports for curve fitting\n",
    "from iminuit import Minuit\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# Keras imports\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout, Flatten, Reshape, Conv2D, Conv2DTranspose, Concatenate, Lambda,BatchNormalization, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.losses import binary_crossentropy\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.python.keras.backend import set_image_data_format\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92a766bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(a, b):\n",
    "    #\n",
    "    num_snaps = 20000\n",
    "    tdiff = 9.0\n",
    "    grid_dataset_list = []\n",
    "    file_index = 1\n",
    "    #alphas = np.linspace(0.2, 0.8, 4)\n",
    "    #betas = np.linspace(0.2, 0.8, 4)\n",
    "    \n",
    "    grid_pt_data = pd.read_csv(\"RealTimeSnaps{}alpha{}beta{}tdiff{}.csv\".format(num_snaps, a, b, tdiff), header=None)\n",
    "    grid_pt_data['y'] = [file_index] * grid_pt_data.shape[0]\n",
    "    grid_dataset_list.append(grid_pt_data)\n",
    "    #print('Alpha, Beta is {} and given label {}'.format((a, b), file_index))\n",
    "    file_index += 1\n",
    "    \n",
    "    grid_dataset = pd.concat(grid_dataset_list)\n",
    "\n",
    "    X = grid_dataset.iloc[:, :-1]\n",
    "    y = grid_dataset.iloc[:, -1]\n",
    "    \n",
    "    return X, y, file_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a634c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    #\n",
    "    def __init__(self, idnum, original_dim=100, intermediate_dim1=75, intermediate_dim2=50, latent_dim=1):\n",
    "        #\n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim1 = intermediate_dim1\n",
    "        self.intermediate_dim2 = intermediate_dim2\n",
    "        self.idnum = idnum\n",
    "        \n",
    "    def sampling(self, args):\n",
    "        # Unpack arguments\n",
    "        z_mean, z_log_var = args\n",
    "        # Get shape of random noise to sample\n",
    "        epsilon = K.random_normal(shape=K.shape(z_mean))\n",
    "        # Return samples from latent space p.d.f.\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def build_vae(self):\n",
    "        #\n",
    "        original_dim = self.original_dim\n",
    "        latent_dim = self.latent_dim\n",
    "        intermediate_dim1 = self.intermediate_dim1\n",
    "        intermediate_dim2 = self.intermediate_dim2\n",
    "        \n",
    "        # encoder\n",
    "        inputs = Input(original_dim, name='input')\n",
    "        x = Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "        x = Dense(intermediate_dim2, activation='relu')(x)\n",
    "        z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim, ), name='z')([z_mean, z_log_var])\n",
    "        self.encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "        \n",
    "        #decoder\n",
    "        latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "        x = Dense(intermediate_dim2, activation='relu')(latent_inputs)\n",
    "        x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "        outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        \n",
    "        i = self.encoder.inputs\n",
    "        if len(i) == 1:\n",
    "            i = i[0]\n",
    "            pass\n",
    "        z = self.encoder(i)[2]\n",
    "        o = self.decoder(z)\n",
    "        self.vae = Model(i, o, name='VAE'+str(self.idnum))\n",
    "        \n",
    "    def compile_vae(self):\n",
    "        # Get the latent p.d.f. mean and log-variance output layers from VAE encoder\n",
    "        encoder   = self.vae.get_layer('encoder')\n",
    "        z_log_var = encoder.get_layer('z_log_var').output\n",
    "        z_mean    = encoder.get_layer('z_mean').output\n",
    "\n",
    "        # Define reconstruction loss\n",
    "        def reco_loss(y_true, y_pred):\n",
    "            # Use binary cross-entropy loss\n",
    "            reco_loss_value = binary_crossentropy(y_true, y_pred) # Averages over axis=-1\n",
    "            reco_loss_value = K.sum(reco_loss_value)\n",
    "            return reco_loss_value\n",
    "\n",
    "        # Define Kullback-Leibler loss with reference to encoder output layers\n",
    "        def kl_loss(y_true, y_pred):\n",
    "            kl_loss_value = 0.5 * (K.square(z_mean) + K.exp(z_log_var) - 1. - z_log_var)\n",
    "            kl_loss_value = K.sum(kl_loss_value, axis=-1)\n",
    "            return kl_loss_value\n",
    "\n",
    "        # Define VAE loss\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            return reco_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "\n",
    "        self.vae.compile(optimizer='adam', loss=vae_loss, metrics=[reco_loss, kl_loss])\n",
    "        return\n",
    "    \n",
    "    def get_summaries(self):\n",
    "        #\n",
    "        return [self.encoder.summary(),\n",
    "                self.decoder.summary(),\n",
    "                self.vae.summary()]\n",
    "    \n",
    "    def get_architectures(self):\n",
    "        #\n",
    "        return [plot_model(self.encoder, show_shapes=True),\n",
    "                plot_model(self.decoder, show_shapes=True),\n",
    "                plot_model(self.vae, show_shapes=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cb374d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_length = 100\n",
    "#zstart = -4.\n",
    "#zstop = 4.\n",
    "#znum = 1000\n",
    "#zrange = np.linspace(zstart, zstop, znum)\n",
    "num_pts = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83eec6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_taus(rhos):\n",
    "    #\n",
    "    taus = np.zeros(lat_length)\n",
    "    for i in range(lat_length):\n",
    "        if random.random() <= rhos[i]:\n",
    "            taus[i] = 1\n",
    "    return taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc93416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mfpoftaus(rhos, taus):\n",
    "    #\n",
    "    return np.prod(rhos**taus * (1-rhos)**(1-taus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9fb41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_complexity1(z_samples, decoder):\n",
    "    avg_1 = 0\n",
    "    for i in range(num_pts):\n",
    "        sample0 = random.choice(z_samples)\n",
    "        rhos0 = decoder.predict(np.array([sample0]))[0]  \n",
    "        taus0 = config_taus(rhos0)\n",
    "        avg_0 = 0\n",
    "        for i in range(num_pts):\n",
    "            sample1 = random.choice(z_samples)\n",
    "            rhos1 = decoder.predict(np.array([sample1]))[0]  \n",
    "            avg_0 += calc_mfpoftaus(rhos1, taus0)\n",
    "        avg_0 /= num_pts\n",
    "        avg_1 += np.log(avg_0)\n",
    "    avg_1 /= num_pts\n",
    "    \n",
    "    avg_2 = 0\n",
    "    for i in range(num_pts):\n",
    "        sample2 = random.choice(z_samples)\n",
    "        rhos2 = decoder.predict(np.array([sample2]))[0]\n",
    "        avg_0b = 0\n",
    "        for i in range(num_pts):\n",
    "            sample_prime = random.choice(z_samples)\n",
    "            rhos_prime = decoder.predict(np.array([sample_prime]))[0]\n",
    "            taus_prime = config_taus(rhos_prime)\n",
    "            avg_0b += np.log(calc_mfpoftaus(rhos2, taus_prime))\n",
    "        avg_0b /= num_pts\n",
    "        avg_2 += avg_0b\n",
    "    avg_2 /= num_pts\n",
    "    \n",
    "    return avg_1 - avg_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9028f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_complexity2(z_samples, decoder):\n",
    "    avg_A = 0\n",
    "    for i in range(num_pts):\n",
    "        sample_A = random.choice(z_samples)\n",
    "        rhos_A = decoder.predict(np.array([sample_A]))[0]\n",
    "        sum_A = np.sum(rhos_A*np.log(rhos_A) + (1-rhos_A)*np.log(1-rhos_A))\n",
    "        avg_A += sum_A\n",
    "    avg_A /= num_pts\n",
    "    \n",
    "    avg_rhos = 0\n",
    "    for i in range(num_pts):\n",
    "        sample_B = random.choice(z_samples)\n",
    "        rhos_B = decoder.predict(np.array([sample_B]))[0]\n",
    "        avg_rhos += rhos_B\n",
    "    avg_rhos /= num_pts\n",
    "    sum_B = np.sum(avg_rhos*np.log(avg_rhos) + (1-avg_rhos)*np.log(1-avg_rhos))\n",
    "    \n",
    "    return avg_A - sum_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "733e902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_experiment():\n",
    "\n",
    "    X_trains = []\n",
    "    X_tests = []\n",
    "    y_trains = []\n",
    "    y_tests = []\n",
    "\n",
    "    X1, y1, file_index1 = load_dataset(0.2, 0.8)\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.5)\n",
    "    X_trains.append(X1_train)\n",
    "    X_tests.append(X1_test)\n",
    "    y_trains.append(y1_train)\n",
    "    y_trains.append(y1_train)\n",
    "\n",
    "    X2, y2, file_index2 = load_dataset(0.4, 0.6)\n",
    "    X2 = pd.concat([X1, X2])\n",
    "    y2 = pd.concat([y1, y2])\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5)\n",
    "    X_trains.append(X2_train)\n",
    "    X_tests.append(X2_test)\n",
    "    y_trains.append(y2_train)\n",
    "    y_trains.append(y2_train)\n",
    "\n",
    "    X3, y3, file_index3 = load_dataset(0.6, 0.4)\n",
    "    X3 = pd.concat([X1, X2, X3])\n",
    "    y3 = pd.concat([y1, y2, y3])\n",
    "    X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.5)\n",
    "    X_trains.append(X3_train)\n",
    "    X_tests.append(X3_test)\n",
    "    y_trains.append(y3_train)\n",
    "    y_trains.append(y3_train)\n",
    "\n",
    "    X4, y4, file_index4 = load_dataset(0.8, 0.2)\n",
    "    X4 = pd.concat([X1, X2, X3, X4])\n",
    "    y4 = pd.concat([y1, y2, y3, y4])\n",
    "    X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.5)\n",
    "    X_trains.append(X4_train)\n",
    "    X_tests.append(X4_test)\n",
    "    y_trains.append(y4_train)\n",
    "    y_trains.append(y4_train)\n",
    "\n",
    "    \n",
    "\n",
    "    VAEs = []\n",
    "    for i in range(len(X_trains)):\n",
    "        Vae = VAE(i)\n",
    "        Vae.build_vae()\n",
    "        Vae.compile_vae()\n",
    "        VAEs.append(Vae)\n",
    "\n",
    "    nb_epochs  = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    histories = []\n",
    "    for i in range(len(VAEs)):\n",
    "        history = VAEs[i].vae.fit(X_trains[i], X_trains[i],\n",
    "                                  epochs=nb_epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  validation_split=0.2,\n",
    "                                  verbose=False)\n",
    "        histories.append(history)\n",
    "\n",
    "    zsamples_list = []\n",
    "    for i in range(len(VAEs)):\n",
    "        _, _, zsamples = VAEs[i].encoder.predict(X_tests[i])\n",
    "        zsamples_list.append(zsamples)\n",
    "\n",
    "    complexities1 = []\n",
    "    for i in range(len(zsamples_list)):\n",
    "        complexity1 = calc_complexity1(zsamples_list[i], VAEs[i].decoder)\n",
    "        complexities1.append(complexity1)\n",
    "        print('Done complexity1: ',i)\n",
    "\n",
    "    complexities2 = []\n",
    "    for i in range(len(zsamples_list)):\n",
    "        complexity2 = calc_complexity2(zsamples_list[i], VAEs[i].decoder)\n",
    "        complexities2.append(complexity2)\n",
    "        print('Done complexity2: ',i)\n",
    "\n",
    "    for i in range(len(complexities1)):\n",
    "        print('{},{},{},{}'.format(i,\n",
    "                                   histories[i].history['val_kl_loss'][-1],\n",
    "                                   complexities1[i],\n",
    "                                   complexities2[i]))\n",
    "\n",
    "    complexities0 = []\n",
    "    for i in range(len(histories)):\n",
    "        complexities0.append(histories[i].history['val_kl_loss'][-1])\n",
    "\n",
    "    # Highest to lowest complexity\n",
    "    c0_ranked = (-np.array(complexities0)).argsort()\n",
    "    c1_ranked = (-np.array(complexities1)).argsort()\n",
    "    c2_ranked = (-np.array(complexities2)).argsort()\n",
    "\n",
    "    print('validation_KL: ',c0_ranked)\n",
    "    print('complexity1: ',c1_ranked)\n",
    "    print('complexity2: ',c2_ranked)\n",
    "    \n",
    "    csvrow = []\n",
    "    for i in range(len(complexities0)):\n",
    "        csvrow.append(complexities0[i])\n",
    "        csvrow.append(complexities1[i])\n",
    "        csvrow.append(complexities2[i])\n",
    "    \n",
    "    return csvrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a02d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45ffcd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  1\n",
      "Done complexity1:  0\n",
      "Done complexity1:  1\n",
      "Done complexity1:  2\n",
      "Done complexity1:  3\n",
      "Done complexity2:  0\n",
      "Done complexity2:  1\n",
      "Done complexity2:  2\n",
      "Done complexity2:  3\n",
      "0,0.00010106316767632961,0.2660458334188007,0.017342643737791263\n",
      "1,0.13462145626544952,0.9065068429389314,0.32657817840576087\n",
      "2,0.6681662201881409,10.501656756748993,5.1008613204956035\n",
      "3,0.8731286525726318,16.93745530992681,9.60566287994385\n",
      "validation_KL:  [3 2 1 0]\n",
      "complexity1:  [3 2 1 0]\n",
      "complexity2:  [3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# write to csv file\n",
    "csvtitle = ['# MF Multiple Point Complexities']\n",
    "csvheader = ['experiment',\n",
    "             'MF1C0', 'MF1C1', 'MF1C2', 'MF2C0', 'MF2C1', 'MF2C2',\n",
    "             'MF3C0', 'MF3C1', 'MF3C2', 'MF4C0', 'MF4C1', 'MF4C2'\n",
    "             ]\n",
    "\n",
    "with open('MF_multipoint_complexity_data.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(csvtitle)\n",
    "    writer.writerow(csvheader)\n",
    "    for i in range(num_experiments):\n",
    "        print('run ', i+1)\n",
    "        writer.writerow([i+1]+ one_experiment())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
